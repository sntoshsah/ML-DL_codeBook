{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import IMDB\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "tar_path = \"../Chapter_15/aclImdb_v1.tar.gz\"\n",
    "extract_path = \"../Chapter_15/aclImdb_v1/\"\n",
    "def tar_extract(tar_path, extract_path):\n",
    "\twith tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "\t    tar.extractall(path=extract_path)\n",
    "tar_extract(tar_path, extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "\tdef __init__(self, data_dir, split=\"train\", transform = None):\n",
    "\t\tself.data_dir = os.path.join(data_dir, split)\n",
    "\t\tself.transform = transform\n",
    "\n",
    "\t\t# store the file paths and labels\n",
    "\t\tself.texts = []\n",
    "\t\tself.labels = []\n",
    "\n",
    "\t\t# load positive reviews\n",
    "\t\tpos_dir = os.path.join(self.data_dir, \"pos\")\n",
    "\t\tfor file in os.listdir(pos_dir):\n",
    "\t\t\twith open(os.path.join(pos_dir, file),'r', encoding=\"utf-8\") as f:\n",
    "\t\t\t\tself.texts.append(f.read())\n",
    "\t\t\t\tself.labels.append(1)  # positive label\n",
    "\n",
    "\t\tneg_dir = os.path.join(self.data_dir, \"neg\")\n",
    "\t\tfor file in os.listdir(neg_dir):\n",
    "\t\t\twith open(os.path.join(neg_dir, file), 'r', encoding=\"utf-8\") as f:\n",
    "\t\t\t\tself.texts.append(f.read())\n",
    "\t\t\t\tself.labels.append(0) # Negative label\n",
    "\t\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.texts)\n",
    "\t\n",
    "\tdef __getitem__(self, index) :\n",
    "\t\tsample = self.texts[index]\n",
    "\t\tlabel = self.labels[index]\n",
    "\n",
    "\t\tif self.transform:\n",
    "\t\t\tsample = self.transform(sample)\n",
    "\n",
    "\t\treturn sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "20000\n",
      "5000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "data_dir = \"../Chapter_15/aclImdb_v1/aclImdb\"\n",
    "train_dataset = IMDBDataset(data_dir, split=\"train\")\n",
    "print(len(train_dataset))\n",
    "train_dataset, valid_dataset = random_split(list(train_dataset), [20000, 5000])\n",
    "print(len(train_dataset))\n",
    "print(len(valid_dataset))\n",
    "\n",
    "test_dataset = IMDBDataset(data_dir, split=\"test\")\n",
    "print(len(test_dataset))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab-Size:  69105\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def tokenizer(text):\n",
    "\ttext = re.sub(\"<[^>]*>\", \"\", text)\n",
    "\temoticons = re.findall(\n",
    "\t\tr\"(?::|;|=)(?:-)?(?:\\)|\\(|D|P)\", text.lower()\n",
    "\t)\n",
    "\ttext = re.sub(r\"[\\W]+\", \" \",text.lower()) + \" \".join(emoticons).replace(\"-\", '')\n",
    "\ttokenized = text.split()\n",
    "\treturn tokenized\n",
    "\n",
    "\n",
    "token_counts = Counter()\n",
    "for label, line in train_dataset:\n",
    "\t# print(label)\n",
    "\t# print(line)\n",
    "\ttokens = tokenizer(label)\n",
    "\ttoken_counts.update(tokens)\n",
    "\n",
    "print(\"Vocab-Size: \", len(token_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 7, 36, 456]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Encoding each unique token into integers\n",
    "from torchtext.vocab import vocab\n",
    "from collections import OrderedDict\n",
    "sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "vocab = vocab(ordered_dict)\n",
    "vocab.insert_token(\"<pad>\", 0)\n",
    "vocab.insert_token(\"<unk>\",1)\n",
    "vocab.set_default_index(1)\n",
    "\n",
    "print([vocab[token] for token in ['this', 'is', 'an','example']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3-A Define the functions for transformation.\n",
    "text_pipeline = lambda x:[vocab[token] for token in tokenizer(x)]\n",
    "label_pipeline = lambda x:1 if x == 'pos' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 10, 558, 708,  ...,   0,   0,   0],\n",
      "        [ 31,  50, 101,  ...,   0,   0,   0],\n",
      "        [ 10, 102,  12,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [ 11,   7,  61,  ...,   0,   0,   0],\n",
      "        [ 10,  84, 215,  ...,   0,   0,   0],\n",
      "        [418,  15,   2,  ...,   0,   0,   0]])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([232, 305,  72, 764, 525, 197,  68, 304])\n"
     ]
    }
   ],
   "source": [
    "# Step 3-B : Wrap the encode and transformation function\n",
    "from torch import nn\n",
    "\n",
    "# Reload train_iter to avoid exhaustion\n",
    "train_dataset, test_dataset = IMDB(split=('train', 'test'))\n",
    "train_dataset, valid_dataset = random_split(list(train_dataset), [20000, 5000])\n",
    "\n",
    "\n",
    "\n",
    "# Collate function for batching\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "\n",
    "    for label, text in batch:\n",
    "        label_list.append(label_pipeline(label))\n",
    "        processed_text = torch.tensor(text_pipeline(text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "\n",
    "    # Pad sequences to the max length in the batch\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True)\n",
    "\n",
    "    return padded_text_list, torch.tensor(label_list), torch.tensor(lengths)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(list(train_dataset), batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(list(test_dataset), batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "# Example batch\n",
    "text_batch, label_batch, length_batch = next(iter(train_dataloader))\n",
    "print(text_batch)\n",
    "print(label_batch)\n",
    "print(length_batch)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 764])\n"
     ]
    }
   ],
   "source": [
    "print(text_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's divide all three datasets into dataloaders with the batch size of 32\n",
    "batch_size = 32\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "test_dl = DataLoader(list(test_dataset), batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.6445,  1.6313,  1.5884],\n",
      "         [-0.1480,  0.8178,  1.7236],\n",
      "         [ 0.3459,  0.5839,  0.3513],\n",
      "         [-0.4800, -0.9664,  1.3269]],\n",
      "\n",
      "        [[ 0.3459,  0.5839,  0.3513],\n",
      "         [-0.1600,  0.2704,  1.6945],\n",
      "         [-0.1480,  0.8178,  1.7236],\n",
      "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Embeddig layers for Sentence encoding\n",
    "embedding = nn.Embedding(\n",
    "\tnum_embeddings=10,\n",
    "\tembedding_dim=3,\n",
    "\tpadding_idx=0\n",
    ")\n",
    "\n",
    "# a batch of 2 samples of 4 indices each\n",
    "text_encoded_input = torch.LongTensor([[1,2,4,5],[4,3,2,0]])\n",
    "print(embedding(text_encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(64, 32, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Building an RNN model\n",
    "class RNN(nn.Module):\n",
    "\tdef __init__(self, input_size, hidden_size) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.rnn = nn.RNN(input_size, hidden_size, num_layers=2, batch_first=True)\n",
    "\t\tself.fc= nn.Linear(hidden_size, 1)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t_, hidden = self.rnn(x)\n",
    "\t\tout = hidden[-1, :,:] # we use the final hidden state from the last hidden layer as the input to the fully connected layer\n",
    "\t\tout = self.fc(out)\n",
    "\t\treturn out\n",
    "\t\n",
    "model = RNN(64, 32)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2653],\n",
       "        [ 0.1856],\n",
       "        [-0.2016],\n",
       "        [-0.0917],\n",
       "        [ 0.0908]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(5,3,64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building an RNN model for the sentiment analysis task.\n",
    "class RNN(nn.Module):\n",
    "\tdef __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "\t\tself.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n",
    "\t\tself.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\t\tself.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "\t\tself.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\tdef forward(self, text, lengths):\n",
    "\t\tout = self.embedding(text)\n",
    "\t\tout = nn.utils.rnn.pack_padded_sequence(\n",
    "\t\t\tout, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True\n",
    "\t\t)\n",
    "\t\tout, (hidden, cell) = self.rnn(out)\n",
    "\t\tout = hidden[-1,:,:]\n",
    "\t\tout = self.fc1(out)\n",
    "\t\tout = self.relu(out)\n",
    "\t\tout = self.fc2(out)\n",
    "\t\tout = self.sigmoid(out)\n",
    "\t\treturn out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size,embed_dim, rnn_hidden_size, fc_hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training of the model on dataset\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "writer = SummaryWriter('runs/IMDB_Movie_Review/',)\n",
    "\n",
    "\n",
    "def train(dataloader, epoch):\n",
    "\tmodel.train()\n",
    "\ttotal_acc , total_loss = 0,0\n",
    "\tfor text_batch, label_batch, lengths in dataloader:\n",
    "\t\ttext_batch, label_batch, lengths = text_batch.to(device), label_batch.to(device), lengths.to(device)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tpred = model(text_batch, lengths)[:,0]\n",
    "\t\tloss = loss_fn(pred, label_batch.float())\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\ttotal_acc += (\n",
    "\t\t\t(pred >= 0.5).float() == label_batch\n",
    "\t\t).float().sum().item()\n",
    "\t\ttotal_loss += loss.item()*label_batch.size(0)\n",
    "\tavg_acc = total_acc/len(dataloader.dataset)\n",
    "\tavg_loss = total_loss/len(dataloader.dataset)\n",
    "\n",
    "\twriter.add_scalar(\"Training Accuracy\", avg_acc, epoch)\n",
    "\twriter.add_scalar(\"Training Loss\", avg_loss, epoch)\n",
    "\n",
    "\n",
    "\treturn avg_acc, avg_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate function to measure the model performance \n",
    "def evaluate(dataloader, epoch):\n",
    "\tmodel.eval()\n",
    "\ttotal_acc, total_loss = 0,0\n",
    "\twith torch.no_grad():\n",
    "\t\tfor text_batch, label_batch, lengths in dataloader:\n",
    "\t\t\ttext_batch, label_batch, lengths = text_batch.to(device), label_batch.to(device), lengths.to(device)\n",
    "\t\t\tpred = model(text_batch, lengths)[:,0]\n",
    "\t\t\tloss = loss_fn(pred, label_batch.float())\n",
    "\t\t\ttotal_acc += (\n",
    "\t\t\t\t(pred >= 0.5).float() == label_batch\n",
    "\t\t\t).float().sum().item()\n",
    "\t\t\ttotal_loss += loss.item()*label_batch.size(0)\n",
    "\tavg_acc = total_acc/len(dataloader.dataset)\n",
    "\tavg_loss = total_loss/len(dataloader.dataset)\n",
    "\n",
    "\twriter.add_scalar(\"Training Accuracy\", avg_acc, epoch)\n",
    "\twriter.add_scalar(\"Training Loss\", avg_loss, epoch)\n",
    "\treturn avg_acc, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train Accuracy: 0.9401 Train Loss: 0.1408\t   Validation Accuracy: 1.0000 Validation Loss: 0.0005\n",
      "Epoch 1 Train Accuracy: 1.0000 Train Loss: 0.0002\t   Validation Accuracy: 1.0000 Validation Loss: 0.0001\n",
      "Epoch 2 Train Accuracy: 1.0000 Train Loss: 0.0001\t   Validation Accuracy: 1.0000 Validation Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "torch.manual_seed(1)\n",
    "for epoch in range(num_epochs):\n",
    "\ttrain_acc , train_loss = train(train_dl, epoch)\n",
    "\tvalid_acc, valid_loss = evaluate(valid_dl, epoch)\n",
    "\tprint(f\"Epoch {epoch} Train Accuracy: {train_acc:.4f} Train Loss: {train_loss:.4f}\\\n",
    "\t   Validation Accuracy: {valid_acc:.4f} Validation Loss: {valid_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy :1.0000\n"
     ]
    }
   ],
   "source": [
    "test_acc, _ = evaluate(test_dl, epoch=1)\n",
    "print(f\"test_accuracy :{test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More on the Bidirectional RNN\n",
    "\"\"\" Here, we will set the bidirectional configuration of the LSTM to True , which will\n",
    "make the recurrent layer pass through the input sequences from both directions, start to end, as well as in the reverse direction.\"\"\"\n",
    "\n",
    "class BidirectionRNN(nn.Module):\n",
    "\tdef __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size ):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.embedding = nn.Embedding(\n",
    "\t\t\tvocab_size, embed_dim, padding_idx=0\n",
    "\t\t)\n",
    "\t\tself.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True, bidirectional=True)\n",
    "\t\tself.fc1 = nn.Linear(rnn_hidden_size*2, fc_hidden_size)\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\t\tself.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "\t\tself.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\tdef forward(self, text, lengths):\n",
    "\t\tout = self.embedding(text)\n",
    "\t\tout = nn.utils.rnn.pack_padded_sequence(\n",
    "\t\t\tout, lengths.cpu().numpy(),batch_first=True,enforce_sorted=False\n",
    "\t\t)\n",
    "\t\t_, (hidden, cell) = self.rnn(out)\n",
    "\t\tout = torch.cat((hidden[-2,:,:],\n",
    "\t\t\t\t   hidden[-1,:,:]), dim=1)\n",
    "\t\tout = self.fc1(out)\n",
    "\t\tout = self.relu(out)\n",
    "\t\tout = self.fc2(out)\n",
    "\t\tout = self.sigmoid(out)\n",
    "\t\treturn out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BidirectionRNN(\n",
       "  (embedding): Embedding(69107, 20, padding_idx=0)\n",
       "  (rnn): LSTM(20, 64, batch_first=True, bidirectional=True)\n",
       "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train Accuracy: 0.0001 Train Loss: 0.7352\t   Validation Accuracy: 0.0000 Validation Loss: 0.7352\n",
      "Epoch 1 Train Accuracy: 0.0001 Train Loss: 0.7352\t   Validation Accuracy: 0.0000 Validation Loss: 0.7352\n",
      "Epoch 2 Train Accuracy: 0.0001 Train Loss: 0.7352\t   Validation Accuracy: 0.0000 Validation Loss: 0.7352\n",
      "Epoch 3 Train Accuracy: 0.0001 Train Loss: 0.7352\t   Validation Accuracy: 0.0000 Validation Loss: 0.7352\n",
      "Epoch 4 Train Accuracy: 0.0001 Train Loss: 0.7352\t   Validation Accuracy: 0.0000 Validation Loss: 0.7352\n",
      "test_accuracy :0.0001\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "model = BidirectionRNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size).to(device)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "\ttrain_acc , train_loss = train(train_dl, epoch)\n",
    "\tvalid_acc, valid_loss = evaluate(valid_dl, epoch)\n",
    "\tprint(f\"Epoch {epoch} Train Accuracy: {train_acc:.4f} Train Loss: {train_loss:.4f}\\\n",
    "\t   Validation Accuracy: {valid_acc:.4f} Validation Loss: {valid_loss:.4f}\")\n",
    "\t\n",
    "test_acc, _ = evaluate(test_dl, epoch=1)\n",
    "print(f\"test_accuracy :{test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
